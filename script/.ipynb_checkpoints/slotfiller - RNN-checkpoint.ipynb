{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import codecs\n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pystruct as pystr\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "\n",
    "import progressbar\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По мотивам:\n",
    "http://deeplearning.net/tutorial/rnnslu.html\n",
    "https://chsasank.github.io/spoken-language-understanding.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('slotfilling-data.json', 'r', encoding='UTF8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    X = [item['chat'] for item in data]\n",
    "    y = []\n",
    "    for item in data:\n",
    "        entities = item['entities']\n",
    "        y_item = {}\n",
    "        for entity in entities:\n",
    "            y_item[entity['title']] = {\n",
    "                'start_pos': entity['start_pos'],\n",
    "                'end_pos': entity['end_pos'],\n",
    "                'text': entity['text']\n",
    "            }\n",
    "\n",
    "        y.append(y_item)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, ans = process_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mix the data\n",
    "perm = np.random.permutation(len(data))\n",
    "data, ans = data[perm], ans[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ВАЛЮТА',\n",
       " 'ВРЕМЯ_ДАТА_СНЯТИЯ',\n",
       " 'ЗА_ГРАНИЦЕЙ',\n",
       " 'МЕСТО_СНЯТИЯ',\n",
       " 'НАЗВАНИЕ_БАНКА',\n",
       " 'НОМЕР_ТЕЛЕФОНА',\n",
       " 'РАЗМЕР_КОМИССИИ',\n",
       " 'СУММА_СНЯТИЯ',\n",
       " 'ТАРИФ_КАРТЫ',\n",
       " 'ТИП_КАРТЫ'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_slots = set([item for y_item in ans for item in list(y_item.keys())])\n",
    "possible_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1: Добрый вечер! У меня, похоже, после трех неправильных вводов ПИНа заблокировалась карта. Что мне делать?\\n2: Тогда коллегам передам, они свяжутся с Вами.\\n',\n",
       " {})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data[0], ans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove special symblos and lower \n",
    "def deleteExtraSymbols(line):\n",
    "    if line:\n",
    "        return re.sub(' +',' ', re.sub(r'[^А-Яа-я0-9€$ ]', u' ', line).lower().rstrip().strip())\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "for dialog in data:\n",
    "    clean_data.append(deleteExtraSymbols(dialog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лемматизация\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Приведем ответы к виду ['О', 'О', ..., 'ИМЯ_СЛОТА', ..., 'О', 'О']\n",
    "counter = 0\n",
    "y = []\n",
    "for i in range(len(clean_data)):\n",
    "    y_vec = ['0'] * len(clean_data[i].split(' '))\n",
    "    for slot in possible_slots:\n",
    "        try:\n",
    "            for slot_word in deleteExtraSymbols(ans[i][slot]['text']).split(' '):\n",
    "                y_vec[clean_data[i].split(' ').index(slot_word)] = slot\n",
    "        except:\n",
    "            pass\n",
    "    y.append(y_vec)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5d7ac824ada1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlem_dialog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdialog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mlem_dialog\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_form\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlem_dialog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlem_dialog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/python3/lib/python3.5/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_parses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/python3/lib/python3.5/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mapply_to_parses\u001b[0;34m(self, word, word_lower, parses)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_form\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         ], key=_score_getter, reverse=True)\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_to_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lem_data = []\n",
    "for dialog in clean_data:\n",
    "    lem_dialog = ''\n",
    "    for word in dialog.split(' '):\n",
    "        p = morph.parse(word)[0]\n",
    "        lem_dialog += p.normal_form + ' '\n",
    "    lem_dialog = lem_dialog[:-1]\n",
    "    lem_data.append(lem_dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wordList =  [[x.lower() for x in re.findall(r\"[\\w']+\", y)] for y in lem_data]\n",
    "idx2w = [] \n",
    "for dialog in lem_data:\n",
    "    for word in dialog.split(' '):\n",
    "        if not word in idx2w:\n",
    "            idx2w.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for dialog in lem_data:\n",
    "    n_dialog = []\n",
    "    for word in dialog.split(' '):\n",
    "        n_dialog.append(idx2w.index(word))\n",
    "    X.append(n_dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2la = []\n",
    "y_num = []\n",
    "for line in y:\n",
    "    y_line = []\n",
    "    for slot in line:\n",
    "        if not slot in idx2la:\n",
    "            idx2la.append(slot)\n",
    "        y_line.append(idx2la.index(slot))\n",
    "    y_num.append(y_line)\n",
    "y = y_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "\n",
    "#model = Word2Vec.load('word2vec/w2v_model_tfidf_size300_window5_mc2.w2v')\n",
    "\n",
    "#import re\n",
    "# Список списокв извлеченных из текстов слов\n",
    "#wordList =  [[x.lower() for x in re.findall(r\"[\\w']+\", y)] for y in lem_data]\n",
    "\n",
    "# Объединение всех слов из выборки в один уникальный список\n",
    "#unique_words = list(set([item for sublist in wordList for item in sublist]))\n",
    "\n",
    "# Формирование списка векторов данных из word2vec\n",
    "#X = []\n",
    "#for dialog in data:\n",
    "#    line = []\n",
    "#    for word in dialog:\n",
    "#        try:\n",
    "#            line.append(model[word])\n",
    "#        except:\n",
    "#            line.append([0] * 300) # 300 - длина вектора в v2w\n",
    "#    X.append(line)\n",
    "\n",
    "# Обратное преобразование\n",
    "#word=model.most_similar(positive=[model['тиньков']],topn=1)\n",
    "#print(word[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conlleval(p, g, w, filename):\n",
    "    '''\n",
    "    INPUT:\n",
    "    p :: predictions\n",
    "    g :: groundtruth\n",
    "    w :: corresponding words\n",
    "\n",
    "    OUTPUT:\n",
    "    filename :: name of the file where the predictions\n",
    "    are written. it will be the input of conlleval.pl script\n",
    "    for computing the performance in terms of precision\n",
    "    recall and f1 score\n",
    "    '''\n",
    "    out = ''\n",
    "    for sl, sp, sw in zip(g, p, w):\n",
    "        out += 'BOS O O\\n'\n",
    "        for wl, wp, w in zip(sl, sp, sw):\n",
    "            out += w + ' ' + wl + ' ' + wp + '\\n'\n",
    "        out += 'EOS O O\\n\\n'\n",
    "\n",
    "    f = open(filename,'w')\n",
    "    f.writelines(out)\n",
    "    f.close()\n",
    "    \n",
    "    return get_perf(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_perf(filename):\n",
    "    ''' run conlleval.pl perl script to obtain\n",
    "    precision/recall and F1 score '''\n",
    "    _conlleval = PREFIX + 'conlleval.pl'\n",
    "    if not isfile(_conlleval):\n",
    "        #download('http://www-etud.iro.umontreal.ca/~mesnilgr/atis/conlleval.pl') \n",
    "        os.system('wget https://www.comp.nus.edu.sg/%7Ekanmy/courses/practicalNLP_2008/packages/conlleval.pl')\n",
    "        chmod('conlleval.pl', stat.S_IRWXU) # give the execute permissions\n",
    "\n",
    "    proc = subprocess.Popen([\"perl\", _conlleval], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, _ = proc.communicate(open(filename,'rb').read())\n",
    "    for line in stdout.decode(\"utf-8\").split('\\n'):\n",
    "        if 'accuracy' in line:\n",
    "            out = line.split()\n",
    "            break\n",
    "    \n",
    "    # out = ['accuracy:', '16.26%;', 'precision:', '0.00%;', 'recall:', '0.00%;', 'FB1:', '0.00']\n",
    "    \n",
    "    precision = float(out[3][:-2])\n",
    "    recall    = float(out[5][:-2])\n",
    "    f1score   = float(out[7])\n",
    "\n",
    "    return {'p':precision, 'r':recall, 'f1':f1score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model\n",
    "n_classes = len(possible_slots) + 1\n",
    "n_vocab = len(idx2w)\n",
    "\n",
    "test = []\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab,100))\n",
    "model.add(Convolution1D(64,5,border_mode='same', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GRU(100,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "model.compile('rmsprop', 'categorical_crossentropy')\n",
    "\n",
    "### Ground truths etc for conlleval\n",
    "X_train, y_train, X_test, y_test = X[:6000], y[:6000], X[6000:], y[6000:]\n",
    "\n",
    "words_val = [ list(map(lambda x: idx2w[x], w)) for w in X_test]\n",
    "groundtruth_val = [ list(map(lambda x: idx2la[x], y)) for y in y_test]\n",
    "words_train = [ list(map(lambda x: idx2w[x], w)) for w in X_train]\n",
    "groundtruth_train = [ list(map(lambda x: idx2la[x], y)) for y in y_train]\n",
    "\n",
    "\n",
    "### Training\n",
    "n_epochs = 10\n",
    "\n",
    "train_f_scores = []\n",
    "val_f_scores = []\n",
    "best_val_f1 = 0\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print(\"Epoch {}\".format(i))\n",
    "    \n",
    "    print(\"Training =>\")\n",
    "    train_pred_label = []\n",
    "    avgLoss = 0\n",
    "    \t\n",
    "    bar = progressbar.ProgressBar(maxval=len(X_train))\n",
    "    for n_batch, sent in bar(enumerate(X_train)):\n",
    "        label = y_train[n_batch]\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "        sent = np.array(sent)\n",
    "        sent = sent[np.newaxis,:]\n",
    "        \n",
    "        if sent.shape[1] > 1: #some bug in keras\n",
    "            loss = model.train_on_batch(sent, label)\n",
    "            avgLoss += loss\n",
    "\n",
    "        pred = model.predict_on_batch(sent)\n",
    "        pred = np.argmax(pred,-1)[0]\n",
    "        train_pred_label.append(pred)\n",
    "\n",
    "    avgLoss = avgLoss/n_batch\n",
    "    \n",
    "    predword_train = [ list(map(lambda x: idx2la[x], y)) for y in train_pred_label]\n",
    "    test = predword_train\n",
    "    con_dict = conlleval(predword_train, groundtruth_train, words_train, 'r.txt')\n",
    "    #con_dict = conlleval(train_pred_label, y_train, X_train, 'r.txt')\n",
    "    train_f_scores.append(con_dict['f1'])\n",
    "    print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['r'], con_dict['p'], con_dict['f1']))\n",
    "    \n",
    "    \n",
    "    print(\"Testing =>\")\n",
    "    \n",
    "    val_pred_label = []\n",
    "    avgLoss = 0\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=len(X_test))\n",
    "    for n_batch, sent in bar(enumerate(X_test)):\n",
    "        label = y_test[n_batch]\n",
    "        label = np.eye(n_classes)[label][np.newaxis,:]\n",
    "        sent = np.array(sent)\n",
    "        sent[np.newaxis, :]\n",
    "        \n",
    "        if sent.shape[1] > 1: #some bug in keras\n",
    "            loss = model.test_on_batch(sent, label)\n",
    "            avgLoss += loss\n",
    "\n",
    "        pred = model.predict_on_batch(sent)\n",
    "        pred = np.argmax(pred,-1)[0]\n",
    "        val_pred_label.append(pred)\n",
    "\n",
    "    avgLoss = avgLoss/n_batch\n",
    "    \n",
    "    predword_val = [ list(map(lambda x: idx2la[x], y)) for y in val_pred_label]\n",
    "    con_dict = conlleval(predword_val, y_test, X_test, 'r.txt')\n",
    "    val_f_scores.append(con_dict['f1'])\n",
    "    \n",
    "    print('Loss = {}, Precision = {}, Recall = {}, F1 = {}'.format(avgLoss, con_dict['r'], con_dict['p'], con_dict['f1']))\n",
    "\n",
    "    if con_dict['f1'] > best_val_f1:\n",
    "    \tbest_val_f1 = con_dict['f1']\n",
    "    \topen('model_architecture.json','w').write(model.to_json())\n",
    "    \tmodel.save_weights('best_model_weights.h5',overwrite=True)\n",
    "    \tprint(\"Best validation F1 score = {}\".format(best_val_f1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con_dict = conlleval(predword_train, groundtruth_train, words_train, 'r.txt')\n",
    "predword_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = X[:6000], y[:6000], X[6000:], y[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleSolutionModel:\n",
    "    def __init__(self):\n",
    "        self._text_to_slot = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for y_item in y:\n",
    "            for slot_title, slot_info in y_item.items():\n",
    "                self._text_to_slot[slot_info['text']] = slot_title\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y = []\n",
    "        \n",
    "        for x_item in X:\n",
    "            y_item = {}\n",
    "            for slot_text, slot_title in self._text_to_slot.items():\n",
    "                index = x_item.find(slot_text)\n",
    "                if index != -1:\n",
    "                    y_item[slot_title] = { \n",
    "                        'start_pos': index, \n",
    "                        'end_pos': index + len(slot_text), \n",
    "                        'text': slot_text\n",
    "                    }\n",
    "                \n",
    "            y.append(y_item)\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleSolutionModel()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(token):\n",
    "    return ''.join([char for char in token if char not in ['.']])\n",
    "\n",
    "def q_distance(tokens_test, tokens_pred):\n",
    "    tokens_test = [tokenize(token) for token in tokens_test]\n",
    "    tokens_pred = [tokenize(token) for token in tokens_pred]\n",
    "    \n",
    "    common = len(set(tokens_test) & set(tokens_pred))\n",
    "    fp = len(set(tokens_pred) - set(tokens_test))\n",
    "    fn = len(set(tokens_test) - set(tokens_pred))\n",
    "    \n",
    "    return common / (common + fp + fn)\n",
    "\n",
    "def precision_on_dataset(X, y, y_pred):\n",
    "    \"\"\"\n",
    "    X_test - array of chats\n",
    "    y_test - hash with slots { 'SLOT_NAME': { 'start_pos': 123, 'end_pos': 135 }, ... }\n",
    "    y_pred - hash_with_predicted_slots\n",
    "    \"\"\"\n",
    "    \n",
    "    q_sum = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x_item, y_item, y_pred_item in tqdm(zip(X, y, y_pred)):\n",
    "        for slot_title, y_pred_slot_info in y_pred_item.items():\n",
    "            if slot_title in y_item:\n",
    "                y_pred_tokens = x_item[y_pred_slot_info['start_pos']:y_pred_slot_info['end_pos']].split(' ')\n",
    "                y_tokens = x_item[y_item[slot_title]['start_pos']:y_item[slot_title]['end_pos']].split(' ')\n",
    "                \n",
    "                q_sum += q_distance(y_tokens, y_pred_tokens)\n",
    "            \n",
    "            total += 1\n",
    "            \n",
    "    return q_sum / total\n",
    "\n",
    "def recall_on_dataset(X, y, y_pred):\n",
    "    \"\"\"\n",
    "    X_test - array of chats\n",
    "    y_test - hash with slots { 'SLOT_NAME': { 'start_pos': 123, 'end_pos': 135 }, ... }\n",
    "    y_pred - hash_with_predicted_slots\n",
    "    \"\"\"\n",
    "    \n",
    "    q_sum = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x_item, y_item, y_pred_item in tqdm(zip(X, y, y_pred)):\n",
    "        for slot_title, y_pred_slot_info in y_item.items():\n",
    "            if slot_title in y_pred_item:\n",
    "                y_pred_tokens = x_item[y_pred_slot_info['start_pos']:y_pred_slot_info['end_pos']].split(' ')\n",
    "                y_tokens = x_item[y_item[slot_title]['start_pos']:y_item[slot_title]['end_pos']].split(' ')\n",
    "                \n",
    "                q_sum += q_distance(y_tokens, y_pred_tokens)\n",
    "            \n",
    "            total += 1\n",
    "            \n",
    "    return q_sum / total\n",
    "\n",
    "def f1_on_dataset(X, y, y_pred):\n",
    "    precision = precision_on_dataset(X, y, y_pred)\n",
    "    recall = recall_on_dataset(X, y, y_pred)\n",
    "    \n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_on_dataset(X_test, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ans)):\n",
    "    print (data[i], ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = np.load('word2vec/w2v_model_tfidf_size300_window5_mc2.w2v.syn0.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('word2vec/w2v_model_tfidf_size300_window5_mc2.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_cosmul(positive=['тинькофф'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[1]['ВАЛЮТА']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
